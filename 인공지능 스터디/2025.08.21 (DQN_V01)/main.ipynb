{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e5e20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CSV] Saved transitions to: toy_transitions.csv\n",
      "[Train] epoch 1/8 | steps 2000 | loss 0.374068\n",
      "[Train] epoch 2/8 | steps 4000 | loss 0.531465\n",
      "[Train] epoch 3/8 | steps 6000 | loss 0.571463\n",
      "[Train] epoch 4/8 | steps 8000 | loss 0.184641\n",
      "[Train] epoch 5/8 | steps 10000 | loss 0.085903\n",
      "[Train] epoch 6/8 | steps 12000 | loss 0.081002\n",
      "[Train] epoch 7/8 | steps 14000 | loss 0.077639\n",
      "[Train] epoch 8/8 | steps 16000 | loss 0.075379\n",
      "[Rollout] Saved policy rollouts to: policy_rollout.csv\n",
      "[Done] model saved as dqn_toy_model.keras\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# DQN (오프라인 학습, retracing 경고 제거 버전)\n",
    "# - CSV: s0, action, reward, s2_0, done\n",
    "# - 고정 배치 크기 + @tf.function(reduce_retracing=True) 훈련 루프\n",
    "# =========================================================\n",
    "import os, csv, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# -----------------------\n",
    "# 설정\n",
    "# -----------------------\n",
    "CSV_PATH        = \"toy_transitions.csv\"\n",
    "ROLLOUT_CSV     = \"policy_rollout.csv\"\n",
    "SEED            = 42\n",
    "GAMMA           = 0.99\n",
    "LR              = 1e-3\n",
    "BATCH_SIZE      = 128           # 고정 배치 크기 (변경 시 input_signature도 함께 바꿔야 함)\n",
    "EPOCH_STEPS     = 2000\n",
    "TARGET_SYNC     = 250\n",
    "HIDDEN          = 64\n",
    "NOISE_STD       = 0.1\n",
    "MAX_EP_LEN      = 20\n",
    "DATA_EPISODES   = 4000\n",
    "INIT_STATE_MIN  = -5.0\n",
    "INIT_STATE_MAX  =  5.0\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.run_functions_eagerly(False)  # 그래프 모드 고정(선택)\n",
    "\n",
    "# -----------------------\n",
    "# 장난감 MDP\n",
    "# -----------------------\n",
    "def env_step(x, a):\n",
    "    x2 = x + float(a) + np.random.normal(0.0, NOISE_STD)\n",
    "    reward = -abs(x2)\n",
    "    done = abs(x2) > 6.0\n",
    "    return x2, reward, done\n",
    "\n",
    "def env_reset():\n",
    "    return np.random.uniform(INIT_STATE_MIN, INIT_STATE_MAX)\n",
    "\n",
    "# -----------------------\n",
    "# 1) CSV 생성\n",
    "# -----------------------\n",
    "def make_toy_csv(path=CSV_PATH, episodes=DATA_EPISODES):\n",
    "    header = [\"s0\", \"action\", \"reward\", \"s2_0\", \"done\"]\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(header)\n",
    "        for _ in range(episodes):\n",
    "            x = env_reset()\n",
    "            for _t in range(MAX_EP_LEN):\n",
    "                a = random.choice([-1, 1])\n",
    "                x2, r, done = env_step(x, a)\n",
    "                w.writerow([x, a, r, x2, int(done)])\n",
    "                x = x2\n",
    "                if done:\n",
    "                    break\n",
    "    print(f\"[CSV] Saved transitions to: {path}\")\n",
    "\n",
    "# -----------------------\n",
    "# 2) CSV 로드\n",
    "# -----------------------\n",
    "def load_csv(path=CSV_PATH):\n",
    "    data = np.loadtxt(path, delimiter=\",\", skiprows=1)\n",
    "    s  = data[:, 0:1].astype(np.float32)  # (N,1)\n",
    "    a  = data[:, 1].astype(np.int32)      # (N,)\n",
    "    r  = data[:, 2].astype(np.float32)    # (N,)\n",
    "    s2 = data[:, 3:4].astype(np.float32)  # (N,1)\n",
    "    d  = data[:, 4].astype(np.float32)    # (N,)\n",
    "    a_idx = ((a + 1) // 2).astype(np.int32)  # -1->0, +1->1\n",
    "    return s, a_idx, r, s2, d\n",
    "\n",
    "# -----------------------\n",
    "# Q 네트워크\n",
    "# -----------------------\n",
    "def make_qnet():\n",
    "    inp = keras.layers.Input(shape=(1,))\n",
    "    x = keras.layers.Dense(HIDDEN, activation=\"relu\")(inp)\n",
    "    x = keras.layers.Dense(HIDDEN, activation=\"relu\")(x)\n",
    "    out = keras.layers.Dense(2, activation=\"linear\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(LR), loss=keras.losses.Huber())\n",
    "    return model\n",
    "\n",
    "# -----------------------\n",
    "# 배치 샘플링 (항상 고정 크기 반환)\n",
    "# -----------------------\n",
    "def minibatch_sampler(N, batch_size=BATCH_SIZE):\n",
    "    idx = np.random.randint(0, N, size=(batch_size,))\n",
    "    return idx\n",
    "\n",
    "# -----------------------\n",
    "# 오프라인 DQN 학습 (커스텀 train_step)\n",
    "# -----------------------\n",
    "def offline_dqn_train(path=CSV_PATH, epochs=5):\n",
    "    s, a, r, s2, d = load_csv(path)\n",
    "    N = s.shape[0]\n",
    "\n",
    "    online = make_qnet()\n",
    "    target = make_qnet()\n",
    "    target.set_weights(online.get_weights())\n",
    "\n",
    "    # -------- 커스텀 훈련 스텝 (고정 signature) --------\n",
    "    @tf.function(\n",
    "        reduce_retracing=True,\n",
    "        input_signature=[\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, 1), dtype=tf.float32),  # sb\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE,),   dtype=tf.int32),    # ab\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE,),   dtype=tf.float32),  # rb\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, 1), dtype=tf.float32),  # s2b\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE,),   dtype=tf.float32),  # db\n",
    "        ],\n",
    "    )\n",
    "    def train_step(sb, ab, rb, s2b, db):\n",
    "        # Q_target(s2,·)\n",
    "        q_next = target(s2b, training=False)                  # (B,2)\n",
    "        max_next = tf.reduce_max(q_next, axis=1)              # (B,)\n",
    "        y = rb + (1.0 - db) * tf.constant(GAMMA, tf.float32) * max_next  # (B,)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_curr = online(sb, training=True)                # (B,2)\n",
    "            # 선택한 행동 a에 해당하는 Q만 추출\n",
    "            idx = tf.stack([tf.range(tf.shape(ab)[0], dtype=tf.int32), ab], axis=1)  # (B,2)\n",
    "            q_sel = tf.gather_nd(q_curr, idx)                 # (B,)\n",
    "            # Huber 손실\n",
    "            loss = tf.keras.losses.huber(y_true=y, y_pred=q_sel)\n",
    "\n",
    "        grads = tape.gradient(loss, online.trainable_variables)\n",
    "        online.optimizer.apply_gradients(zip(grads, online.trainable_variables))\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "\n",
    "    global_step = 0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        losses = []\n",
    "        for _ in range(EPOCH_STEPS):\n",
    "            idx = minibatch_sampler(N, BATCH_SIZE)\n",
    "            sb  = s[idx]\n",
    "            ab  = a[idx]\n",
    "            rb  = r[idx]\n",
    "            s2b = s2[idx]\n",
    "            db  = d[idx]\n",
    "\n",
    "            # numpy -> tf.Tensor (dtype/shape 고정)\n",
    "            sb  = tf.convert_to_tensor(sb,  dtype=tf.float32)   # (B,1)\n",
    "            ab  = tf.convert_to_tensor(ab,  dtype=tf.int32)     # (B,)\n",
    "            rb  = tf.convert_to_tensor(rb,  dtype=tf.float32)   # (B,)\n",
    "            s2b = tf.convert_to_tensor(s2b, dtype=tf.float32)   # (B,1)\n",
    "            db  = tf.convert_to_tensor(db,  dtype=tf.float32)   # (B,)\n",
    "\n",
    "            loss = float(train_step(sb, ab, rb, s2b, db).numpy())\n",
    "            losses.append(loss)\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % TARGET_SYNC == 0:\n",
    "                target.set_weights(online.get_weights())\n",
    "\n",
    "        print(f\"[Train] epoch {ep}/{epochs} | steps {global_step} | loss {np.mean(losses):.6f}\")\n",
    "\n",
    "    return online\n",
    "\n",
    "# -----------------------\n",
    "# 정책 롤아웃 (평가)\n",
    "# -----------------------\n",
    "def greedy_action_from_q(qvec):\n",
    "    a_idx = int(np.argmax(qvec))\n",
    "    return -1 if a_idx == 0 else +1\n",
    "\n",
    "def rollout(policy_model, episodes=20, path=ROLLOUT_CSV):\n",
    "    header = [\"episode\", \"t\", \"x\", \"action\", \"x2\", \"reward\", \"done\"]\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(header)\n",
    "        for ep in range(1, episodes + 1):\n",
    "            x = env_reset()\n",
    "            for t in range(1, MAX_EP_LEN + 1):\n",
    "                q = policy_model.predict(np.array([[x]], dtype=np.float32), verbose=0)[0]\n",
    "                a = greedy_action_from_q(q)\n",
    "                x2, r, done = env_step(x, a)\n",
    "                w.writerow([ep, t, x, a, x2, r, int(done)])\n",
    "                x = x2\n",
    "                if done:\n",
    "                    break\n",
    "    print(f\"[Rollout] Saved policy rollouts to: {path}\")\n",
    "\n",
    "# -----------------------\n",
    "# 메인\n",
    "# -----------------------\n",
    "def main():\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        make_toy_csv(CSV_PATH, episodes=DATA_EPISODES)\n",
    "\n",
    "    model = offline_dqn_train(CSV_PATH, epochs=8)\n",
    "    rollout(model, episodes=50, path=ROLLOUT_CSV)\n",
    "    model.save(\"dqn_toy_model.keras\")\n",
    "    print(\"[Done] model saved as dqn_toy_model.keras\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1af9bb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 3.00 -> Q=[-23.015701 -18.664534] -> greedy action=1\n",
      "xs: [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\n",
      "Q(xs):\n",
      " [[ -8.60376  -18.175146]\n",
      " [-14.46262  -21.285158]\n",
      " [-18.480413 -23.066622]\n",
      " [-21.44772  -23.476557]\n",
      " [-23.136992 -23.361061]\n",
      " [-23.66525  -23.828676]\n",
      " [-23.532907 -22.914711]\n",
      " [-23.79907  -21.384521]\n",
      " [-23.015703 -18.664534]\n",
      " [-21.457558 -14.85654 ]\n",
      " [-18.816319  -9.753853]]\n",
      "greedy actions: [-1 -1 -1 -1 -1 -1  1  1  1  1  1]\n",
      "[Saved] Rollout to: policy_rollout_from_saved.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Saved DQN Model Loader & Rollout (for toy MDP)\n",
    "# - dqn_toy_model.keras 불러와서 정책 실행\n",
    "# - 결과를 policy_rollout_from_saved.csv 로 저장\n",
    "# - 단일 상태/배치 상태에 대한 Q값, 행동 추론 함수 포함\n",
    "# =========================================================\n",
    "# pip install tensorflow\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# ----- 장난감 MDP 설정 (이전과 동일) -----\n",
    "NOISE_STD = 0.1\n",
    "INIT_STATE_MIN, INIT_STATE_MAX = -5.0, 5.0\n",
    "MAX_EP_LEN = 20\n",
    "\n",
    "def env_step(x, a):\n",
    "    \"\"\" x(실수), a∈{-1,+1} -> x', r, done \"\"\"\n",
    "    x2 = x + float(a) + np.random.normal(0.0, NOISE_STD)\n",
    "    reward = -abs(x2)            # 원점(0)에 가까울수록 보상↑\n",
    "    done = abs(x2) > 6.0         # 영역 이탈 시 종료\n",
    "    return x2, reward, done\n",
    "\n",
    "def env_reset():\n",
    "    return np.random.uniform(INIT_STATE_MIN, INIT_STATE_MAX)\n",
    "\n",
    "# ----- 행동 매핑 -----\n",
    "# Q네트워크 출력은 길이 2 벡터: index 0 -> action -1, index 1 -> action +1\n",
    "def idx_to_action(a_idx: int) -> int:\n",
    "    return -1 if a_idx == 0 else +1\n",
    "\n",
    "def action_to_idx(a: int) -> int:\n",
    "    # -1 -> 0, +1 -> 1\n",
    "    return 0 if a == -1 else 1\n",
    "\n",
    "# ----- 모델 불러오기 -----\n",
    "def load_policy_model(path=\"dqn_toy_model.keras\"):\n",
    "    model = keras.models.load_model(path)\n",
    "    return model\n",
    "\n",
    "# ----- 단일 상태에서 Q값/행동 추론 -----\n",
    "def q_values_for_state(model, x: float):\n",
    "    \"\"\" 상태 x(스칼라)에 대한 Q(s,·) -> 길이 2 numpy 배열 \"\"\"\n",
    "    s = np.array([[x]], dtype=np.float32)  # (1,1)\n",
    "    q = model.predict(s, verbose=0)[0]     # (2,)\n",
    "    return q\n",
    "\n",
    "def greedy_action(model, x: float) -> int:\n",
    "    q = q_values_for_state(model, x)\n",
    "    a_idx = int(np.argmax(q))\n",
    "    return idx_to_action(a_idx)\n",
    "\n",
    "# ----- 배치 상태에서 Q/행동 추론 -----\n",
    "def q_values_for_states(model, xs):\n",
    "    \"\"\" xs: (N,) 또는 (N,1) -> Q: (N,2) \"\"\"\n",
    "    xs = np.array(xs, dtype=np.float32).reshape(-1, 1)\n",
    "    q = model.predict(xs, verbose=0)  # (N,2)\n",
    "    return q\n",
    "\n",
    "def greedy_actions(model, xs):\n",
    "    \"\"\" xs: (N,) -> actions: (N,) with values in {-1,+1} \"\"\"\n",
    "    q = q_values_for_states(model, xs)      # (N,2)\n",
    "    a_idx = np.argmax(q, axis=1)            # (N,)\n",
    "    acts = np.where(a_idx == 0, -1, +1)     # (N,)\n",
    "    return acts\n",
    "\n",
    "# ----- 정책 롤아웃 & CSV 저장 -----\n",
    "def rollout_to_csv(model, episodes=20, out_csv=\"policy_rollout_from_saved.csv\"):\n",
    "    header = [\"episode\", \"t\", \"x\", \"action\", \"x2\", \"reward\", \"done\"]\n",
    "    with open(out_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(header)\n",
    "        for ep in range(1, episodes + 1):\n",
    "            x = env_reset()\n",
    "            for t in range(1, MAX_EP_LEN + 1):\n",
    "                a = greedy_action(model, x)\n",
    "                x2, r, done = env_step(x, a)\n",
    "                w.writerow([ep, t, x, a, x2, r, int(done)])\n",
    "                x = x2\n",
    "                if done:\n",
    "                    break\n",
    "    print(f\"[Saved] Rollout to: {out_csv}\")\n",
    "\n",
    "# ----- 데모 -----\n",
    "if __name__ == \"__main__\":\n",
    "    model = load_policy_model(\"dqn_toy_model.keras\")\n",
    "\n",
    "    # 1) 단일 상태 추론 예시\n",
    "    x0 = 3.0\n",
    "    q = q_values_for_state(model, x0)\n",
    "    a = greedy_action(model, x0)\n",
    "    print(f\"State {x0:.2f} -> Q={q} -> greedy action={a}\")\n",
    "\n",
    "    # 2) 배치 상태 추론 예시\n",
    "    xs = np.linspace(-5, 5, num=11)  # -5,-4,...,5\n",
    "    qs = q_values_for_states(model, xs)\n",
    "    acts = greedy_actions(model, xs)\n",
    "    print(\"xs:\", xs)\n",
    "    print(\"Q(xs):\\n\", qs)\n",
    "    print(\"greedy actions:\", acts)\n",
    "\n",
    "    # 3) 정책 롤아웃 CSV로 저장\n",
    "    rollout_to_csv(model, episodes=50, out_csv=\"policy_rollout_from_saved.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37d9ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 -1\n",
      "-1.0 -1\n",
      "0.0 -1\n",
      "1.0 1\n",
      "3.0 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model = keras.models.load_model(\"dqn_toy_model.keras\")\n",
    "\n",
    "def greedy_action(x):\n",
    "    q = model.predict(np.array([[x]], dtype=np.float32), verbose=0)[0]\n",
    "    return -1 if np.argmax(q) == 0 else +1\n",
    "\n",
    "for x in [-3.0, -1.0, 0.0, 1.0, 3.0]:\n",
    "    print(x, greedy_action(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "876995fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 50\n",
      "Avg return: -20.192 ± 5.069\n",
      "Avg length: 4.24 steps\n",
      "Best return: -11.022  |  Worst return: -28.118\n"
     ]
    }
   ],
   "source": [
    "import csv, numpy as np\n",
    "\n",
    "rollout_csv = \"policy_rollout.csv\"  # 또는 policy_rollout_from_saved.csv\n",
    "returns = []\n",
    "lengths = []\n",
    "cur_ep, cur_ret, cur_len = None, 0.0, 0\n",
    "\n",
    "with open(rollout_csv, newline=\"\") as f:\n",
    "    r = csv.DictReader(f)\n",
    "    for row in r:\n",
    "        ep = int(row[\"episode\"])\n",
    "        if cur_ep is None:\n",
    "            cur_ep = ep\n",
    "        if ep != cur_ep:\n",
    "            returns.append(cur_ret)\n",
    "            lengths.append(cur_len)\n",
    "            cur_ep, cur_ret, cur_len = ep, 0.0, 0\n",
    "        cur_ret += float(row[\"reward\"])\n",
    "        cur_len += 1\n",
    "    if cur_ep is not None:\n",
    "        returns.append(cur_ret)\n",
    "        lengths.append(cur_len)\n",
    "\n",
    "print(f\"Episodes: {len(returns)}\")\n",
    "print(f\"Avg return: {np.mean(returns):.3f} ± {np.std(returns):.3f}\")\n",
    "print(f\"Avg length: {np.mean(lengths):.2f} steps\")\n",
    "print(f\"Best return: {np.max(returns):.3f}  |  Worst return: {np.min(returns):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
